{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3/4: Full Data Collection, Cleaning, and Integration\n",
    "\n",
    "**Week 3/4: Full Data Collection**\n",
    "- Execute large-scale API data collection for top 500 cities with error handling\n",
    "- Validate SimpleMaps dataset for clean identifiers and reliable population data\n",
    "- Store raw and processed files with consistent naming conventions\n",
    "- Record timestamps, logs, and collection notes\n",
    "- Standardize field names, formats, and units\n",
    "- Merge population, air quality, and weather data\n",
    "- Run validation checks for duplicates, missing values, inconsistent joins\n",
    "- Document cleaning and integration steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from config import API_KEY, BASE_URL\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create log list to track all operations (claude assisted)\n",
    "curation_log = []\n",
    "\n",
    "def log_step(step_name, details):\n",
    "    \"\"\"Log a curation step with timestamp\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    log_entry = {\n",
    "        'timestamp': timestamp,\n",
    "        'step': step_name,\n",
    "        'details': details\n",
    "    }\n",
    "    curation_log.append(log_entry)\n",
    "    print(f\"[{timestamp}] {step_name}: {details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: SimpleMaps Dataset Validation\n",
    "\n",
    "Load and validate the SimpleMaps dataset for data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:26:27.387966] Data Load: Loaded 48059 cities from SimpleMaps dataset\n",
      "Shape: (48059, 11)\n",
      "      city city_ascii     lat       lng    country iso2 iso3 admin_name  \\\n",
      "0    Tokyo      Tokyo  35.687  139.7495      Japan   JP  JPN      Tōkyō   \n",
      "1  Jakarta    Jakarta  -6.175  106.8275  Indonesia   ID  IDN    Jakarta   \n",
      "2    Delhi      Delhi  28.610   77.2300      India   IN  IND      Delhi   \n",
      "\n",
      "   capital  population          id  \n",
      "0  primary  37785000.0  1392685764  \n",
      "1  primary  33756000.0  1360771077  \n",
      "2    admin  32226000.0  1356872604  \n"
     ]
    }
   ],
   "source": [
    "# Load the full SimpleMaps dataset\n",
    "cities_df = pd.read_csv('data/worldcities.csv')\n",
    "log_step('Data Load', f'Loaded {len(cities_df)} cities from SimpleMaps dataset')\n",
    "print(f\"Shape: {cities_df.shape}\")\n",
    "print(cities_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_ascii        2\n",
      "iso2             33\n",
      "admin_name      201\n",
      "capital       32921\n",
      "population      251\n",
      "dtype: int64\n",
      "[2025-11-20T14:26:46.632955] Validation - Missing Values: 5 columns have missing values\n"
     ]
    }
   ],
   "source": [
    "# Check missing\n",
    "missing_data = cities_df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "missing_summary = missing_data[missing_data > 0].to_dict()\n",
    "\n",
    "# log\n",
    "log_step('Validation - Missing Values', f'{len(missing_summary)} columns have missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 3694\n",
      "           city        country      lat       lng\n",
      "37     Dongguan          China  23.0210  113.7520\n",
      "56       Jining          China  35.4151  116.5871\n",
      "63       Fuyang          China  32.8900  115.8140\n",
      "77     Shaoyang          China  27.2395  111.4679\n",
      "86        Miami  United States  25.7840  -80.2101\n",
      "94       Dallas  United States  32.7935  -96.7667\n",
      "116      Suzhou          China  33.6480  116.9640\n",
      "120   Xiangyang          China  32.0100  112.1220\n",
      "123  Washington  United States  38.9047  -77.0163\n",
      "128      Yichun          China  27.8160  114.4170\n",
      "[2025-11-20T14:28:21.702501] Validation - Duplicates: Found 3694 duplicate city-country combinations\n"
     ]
    }
   ],
   "source": [
    "# duplicates\n",
    "# log\n",
    "duplicate_cities = cities_df.duplicated(subset=['city', 'country'], keep=False)\n",
    "num_duplicates = duplicate_cities.sum()\n",
    "\n",
    "print(f\"Duplicates: {num_duplicates}\")\n",
    "if num_duplicates > 0:\n",
    "    print(cities_df[duplicate_cities][['city', 'country', 'lat', 'lng']].head(10))\n",
    "\n",
    "# log\n",
    "log_step('Validation - Duplicates', f'Found {num_duplicates} duplicate city-country combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population coverage: 99.5%\n",
      "Missing: 251\n",
      "count    4.780800e+04\n",
      "mean     1.078566e+05\n",
      "std      6.855111e+05\n",
      "min      0.000000e+00\n",
      "25%      1.219100e+04\n",
      "50%      2.091350e+04\n",
      "75%      4.680850e+04\n",
      "max      3.778500e+07\n",
      "Name: population, dtype: float64\n",
      "[2025-11-20T14:28:32.216926] Validation - Population Data: 99.5% of cities have population data\n"
     ]
    }
   ],
   "source": [
    "# Population data check\n",
    "cities_with_pop = cities_df[cities_df['population'].notna()]\n",
    "pop_coverage = len(cities_with_pop) / len(cities_df) * 100\n",
    "\n",
    "print(f\"Population coverage: {pop_coverage:.1f}%\")\n",
    "print(f\"Missing: {cities_df['population'].isna().sum()}\")\n",
    "print(cities_with_pop['population'].describe())\n",
    "\n",
    "# log\n",
    "log_step('Validation - Population Data', f'{pop_coverage:.1f}% of cities have population data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid coords: 0\n",
      "[2025-11-20T14:28:54.540540] Validation - Coordinates: 0 cities with invalid coordinates\n"
     ]
    }
   ],
   "source": [
    "# Check coordinate validity\n",
    "invalid_coords = cities_df[\n",
    "    (cities_df['lat'].isna()) | \n",
    "    (cities_df['lng'].isna()) |\n",
    "    (cities_df['lat'] < -90) | \n",
    "    (cities_df['lat'] > 90) |\n",
    "    (cities_df['lng'] < -180) | \n",
    "    (cities_df['lng'] > 180)\n",
    "]\n",
    "\n",
    "print(f\"Invalid coords: {len(invalid_coords)}\")\n",
    "if len(invalid_coords) > 0:\n",
    "    print(invalid_coords[['city', 'country', 'lat', 'lng']].head())\n",
    "\n",
    "# log\n",
    "log_step('Validation - Coordinates', f'{len(invalid_coords)} cities with invalid coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop range: 1,543,000 to 37,785,000\n",
      "          city       country  population\n",
      "0        Tokyo         Japan  37785000.0\n",
      "1      Jakarta     Indonesia  33756000.0\n",
      "2        Delhi         India  32226000.0\n",
      "3    Guangzhou         China  26940000.0\n",
      "4       Mumbai         India  24973000.0\n",
      "5       Manila   Philippines  24922000.0\n",
      "6     Shanghai         China  24073000.0\n",
      "7    São Paulo        Brazil  23086000.0\n",
      "8        Seoul  Korea, South  23016000.0\n",
      "9  Mexico City        Mexico  21804000.0\n",
      "[2025-11-20T14:29:13.695526] Data Selection: Selected and saved top 500 cities by population\n"
     ]
    }
   ],
   "source": [
    "# Select top 500 cities by population\n",
    "valid_cities = cities_df[\n",
    "    (cities_df['population'].notna()) &\n",
    "    (cities_df['lat'].notna()) &\n",
    "    (cities_df['lng'].notna())\n",
    "].copy()\n",
    "\n",
    "top_500_cities = valid_cities.nlargest(500, 'population').reset_index(drop=True)\n",
    "\n",
    "print(f\"Pop range: {top_500_cities['population'].min():,.0f} to {top_500_cities['population'].max():,.0f}\")\n",
    "print(top_500_cities[['city', 'country', 'population']].head(10))\n",
    "\n",
    "# Save validated top 500 cities\n",
    "top_500_cities.to_csv('data/raw_top_500_cities.csv', index=False)\n",
    "\n",
    "# log\n",
    "log_step('Data Selection', 'Selected and saved top 500 cities by population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Large-Scale API Data Collection\n",
    "\n",
    "Collect air quality data for all 500 cities with proper error handling and rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_air_quality(lat, lon, api_key, retry_count=3):\n",
    "    \"\"\"Get current air quality with retry logic and error handling\"\"\"\n",
    "    url = f\"{BASE_URL}currentConditions:lookup\"\n",
    "    params = {\"key\": api_key}\n",
    "    data = {\n",
    "        \"location\": {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            response = requests.post(url, params=params, json=data, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return {'status': 'success', 'data': response.json()}\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limit hit, wait longer\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                continue\n",
    "            else:\n",
    "                return {'status': 'error', 'error_type': 'http_error', 'code': response.status_code}\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            return {'status': 'error', 'error_type': 'timeout'}\n",
    "        except Exception as e:\n",
    "            return {'status': 'error', 'error_type': 'exception', 'message': str(e)}\n",
    "    \n",
    "    return {'status': 'error', 'error_type': 'max_retries'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for 500 cities:\n",
      "[2025-11-20T14:38:30.595584] API Collection Start: Beginning collection for 500 cities\n",
      "50/500\n",
      "100/500\n",
      "150/500\n",
      "200/500\n",
      "250/500\n",
      "300/500\n",
      "350/500\n",
      "400/500\n",
      "450/500\n",
      "500/500\n",
      "Done! 418.741052 seconds)\n",
      "[2025-11-20T14:45:29.336727] API Collection Complete: Collected 500 records in 418.7s, 54 errors\n"
     ]
    }
   ],
   "source": [
    "# Initialize collection tracking\n",
    "collection_start = datetime.now()\n",
    "air_quality_data = []\n",
    "error_log = []\n",
    "\n",
    "print(f\"Collecting data for 500 cities:\")\n",
    "# log\n",
    "log_step('API Collection Start', f'Beginning collection for {len(top_500_cities)} cities')\n",
    "\n",
    "# all 500\n",
    "for idx, row in top_500_cities.iterrows():\n",
    "    city_name = row['city']\n",
    "    country = row['country']\n",
    "    lat = row['lat']\n",
    "    lon = row['lng']\n",
    "    population = row['population']\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"{idx + 1}/{len(top_500_cities)}\")\n",
    "    result = get_current_air_quality(lat, lon, API_KEY)\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        api_data = result['data']\n",
    "        \n",
    "        aqi = None\n",
    "        category = None\n",
    "        dominant_pollutant = None\n",
    "        \n",
    "        if 'indexes' in api_data and len(api_data['indexes']) > 0:\n",
    "            index_data = api_data['indexes'][0]\n",
    "            aqi = index_data.get('aqi', None)\n",
    "            category = index_data.get('category', None)\n",
    "            dominant_pollutant = index_data.get('dominantPollutant', None)\n",
    "        \n",
    "        air_quality_data.append({\n",
    "            'city': city_name,\n",
    "            'country': country,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'aqi': aqi,\n",
    "            'aqi_category': category,\n",
    "            'dominant_pollutant': dominant_pollutant,\n",
    "            'collection_timestamp': datetime.now().isoformat(),\n",
    "            'status': 'success'\n",
    "        })\n",
    "    else:\n",
    "        # error log\n",
    "        error_log.append({\n",
    "            'city': city_name,\n",
    "            'country': country,\n",
    "            'error_type': result.get('error_type', 'unknown'),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        air_quality_data.append({\n",
    "            'city': city_name,\n",
    "            'country': country,\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'aqi': None,\n",
    "            'aqi_category': None,\n",
    "            'dominant_pollutant': None,\n",
    "            'collection_timestamp': datetime.now().isoformat(),\n",
    "            'status': 'error'\n",
    "        })\n",
    "    \n",
    "    # rate limit\n",
    "    time.sleep(0.25)\n",
    "\n",
    "collection_end = datetime.now()\n",
    "duration = (collection_end - collection_start).total_seconds()\n",
    "\n",
    "print(f\"Done! {duration} seconds)\")\n",
    "\n",
    "# log\n",
    "log_step('API Collection Complete', f'Collected {len(air_quality_data)} records in {duration:.1f}s, {len(error_log)} errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:45:59.477932] Data Storage: Saved raw API data: data/raw_air_quality_20251120_144559.csv\n",
      "54\n",
      "Saved errors: data/collection_errors_20251120_144559.csv\n",
      "[2025-11-20T14:45:59.480788] Error Logging: Saved 54 errors to data/collection_errors_20251120_144559.csv\n"
     ]
    }
   ],
   "source": [
    "# raw data\n",
    "raw_aq_df = pd.DataFrame(air_quality_data)\n",
    "raw_filename = f\"data/raw_air_quality_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "raw_aq_df.to_csv(raw_filename, index=False)\n",
    "\n",
    "# log\n",
    "log_step('Data Storage', f'Saved raw API data: {raw_filename}')\n",
    "\n",
    "print(len(error_log))\n",
    "# Save error log if there are errors\n",
    "if len(error_log) > 0:\n",
    "    error_df = pd.DataFrame(error_log)\n",
    "    error_filename = f\"data/collection_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    error_df.to_csv(error_filename, index=False)\n",
    "    print(f\"Saved errors: {error_filename}\")\n",
    "    # log\n",
    "    log_step('Error Logging', f'Saved {len(error_log)} errors to {error_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Cleaning and Standardization\n",
    "\n",
    "Standardize field names, formats, and units across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:46:38.559416] Cleaning Start: Beginning data cleaning and standardization\n"
     ]
    }
   ],
   "source": [
    "# Load raw data for cleaning\n",
    "cities_clean = top_500_cities.copy()\n",
    "aq_clean = raw_aq_df.copy()\n",
    "\n",
    "# log\n",
    "log_step('Cleaning Start', 'Beginning data cleaning and standardization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:47:04.684477] Cleaning - Column Names: Standardized column names to snake_case\n"
     ]
    }
   ],
   "source": [
    "# Standardize column names\n",
    "cities_clean = cities_clean.rename(columns={\n",
    "    'lat': 'latitude',\n",
    "    'lng': 'longitude'\n",
    "})\n",
    "\n",
    "aq_clean = aq_clean.rename(columns={\n",
    "    'lat': 'latitude',\n",
    "    'lon': 'longitude'\n",
    "})\n",
    "\n",
    "# log\n",
    "log_step('Cleaning - Column Names', 'Standardized column names to snake_case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:47:06.899844] Cleaning - Data Types: Standardized numeric and string data types\n"
     ]
    }
   ],
   "source": [
    "# Standardize data types\n",
    "cities_clean['population'] = pd.to_numeric(cities_clean['population'], errors='coerce')\n",
    "cities_clean['latitude'] = pd.to_numeric(cities_clean['latitude'], errors='coerce')\n",
    "cities_clean['longitude'] = pd.to_numeric(cities_clean['longitude'], errors='coerce')\n",
    "\n",
    "aq_clean['aqi'] = pd.to_numeric(aq_clean['aqi'], errors='coerce')\n",
    "aq_clean['latitude'] = pd.to_numeric(aq_clean['latitude'], errors='coerce')\n",
    "aq_clean['longitude'] = pd.to_numeric(aq_clean['longitude'], errors='coerce')\n",
    "\n",
    "cities_clean['city'] = cities_clean['city'].str.strip()\n",
    "cities_clean['country'] = cities_clean['country'].str.strip()\n",
    "\n",
    "aq_clean['city'] = aq_clean['city'].str.strip()\n",
    "aq_clean['country'] = aq_clean['country'].str.strip()\n",
    "\n",
    "# log\n",
    "log_step('Cleaning - Data Types', 'Standardized numeric and string data types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing AQI: 54 (10.8%)\n",
      "data_quality_flag\n",
      "complete       446\n",
      "missing_aqi     54\n",
      "Name: count, dtype: int64\n",
      "[2025-11-20T14:59:08.994920] Cleaning - Missing Values: 54 cities missing AQI data, flagged for transparency\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "missing_aqi = aq_clean['aqi'].isna().sum()\n",
    "print(f\"Missing AQI: {missing_aqi} ({missing_aqi/len(aq_clean)*100:.1f}%)\")\n",
    "\n",
    "# Flag data quality\n",
    "aq_clean['data_quality_flag'] = aq_clean['aqi'].notna().map({True: 'complete', False: 'missing_aqi'})\n",
    "print(aq_clean['data_quality_flag'].value_counts())\n",
    "\n",
    "# log\n",
    "log_step('Cleaning - Missing Values', f'{missing_aqi} cities missing AQI data, flagged for transparency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQI categories: ['Good air quality' 'Poor air quality' 'Excellent air quality'\n",
      " 'Low air quality' 'Moderate air quality' None]\n",
      "Pollutants: ['pm25' 'pm10' 'o3' None 'no2']\n",
      "[2025-11-20T14:59:20.358595] Cleaning - Categories: Reviewed and standardized categorical values\n"
     ]
    }
   ],
   "source": [
    "# Check categorical values\n",
    "print(f\"AQI categories: {aq_clean['aqi_category'].unique()}\")\n",
    "print(f\"Pollutants: {aq_clean['dominant_pollutant'].unique()}\")\n",
    "\n",
    "log_step('Cleaning - Categories', 'Reviewed and standardized categorical values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Integration\n",
    "\n",
    "Merge population and air quality data into a single integrated table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T14:59:41.440665] Integration Start: Beginning merge of population and air quality data\n"
     ]
    }
   ],
   "source": [
    "# log\n",
    "log_step('Integration Start', 'Beginning merge of population and air quality data')\n",
    "\n",
    "# Select relevant columns from each dataset\n",
    "cities_cols = ['city', 'country', 'latitude', 'longitude', 'population', 'iso2', 'iso3']\n",
    "aq_cols = ['city', 'country', 'aqi', 'aqi_category', 'dominant_pollutant', \n",
    "           'collection_timestamp', 'data_quality_flag']\n",
    "\n",
    "cities_for_merge = cities_clean[cities_cols]\n",
    "aq_for_merge = aq_clean[aq_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_merge\n",
      "both          508\n",
      "left_only       0\n",
      "right_only      0\n",
      "Name: count, dtype: int64\n",
      "Shape: (508, 13)\n",
      "[2025-11-20T14:59:53.117175] Integration - Merge: Merged 508 records\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "integrated_data = cities_for_merge.merge(\n",
    "    aq_for_merge,\n",
    "    on=['city', 'country'],\n",
    "    how='left',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "print(integrated_data['_merge'].value_counts())\n",
    "print(f\"Shape: {integrated_data.shape}\")\n",
    "\n",
    "# log\n",
    "log_step('Integration - Merge', f'Merged {len(integrated_data)} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city    country  latitude  longitude  population iso2 iso3   aqi  \\\n",
      "0      Tokyo      Japan   35.6870   139.7495  37785000.0   JP  JPN  64.0   \n",
      "1    Jakarta  Indonesia   -6.1750   106.8275  33756000.0   ID  IDN  63.0   \n",
      "2      Delhi      India   28.6100    77.2300  32226000.0   IN  IND  14.0   \n",
      "3  Guangzhou      China   23.1300   113.2600  26940000.0   CN  CHN  60.0   \n",
      "4     Mumbai      India   19.0761    72.8775  24973000.0   IN  IND  19.0   \n",
      "\n",
      "       aqi_category dominant_pollutant        collection_timestamp  \\\n",
      "0  Good air quality               pm25  2025-11-20T14:38:31.279676   \n",
      "1  Good air quality               pm10  2025-11-20T14:38:32.165895   \n",
      "2  Poor air quality               pm10  2025-11-20T14:38:32.973334   \n",
      "3  Good air quality               pm10  2025-11-20T14:38:33.801853   \n",
      "4  Poor air quality               pm10  2025-11-20T14:38:34.638730   \n",
      "\n",
      "  data_quality_flag  \n",
      "0          complete  \n",
      "1          complete  \n",
      "2          complete  \n",
      "3          complete  \n",
      "4          complete  \n"
     ]
    }
   ],
   "source": [
    "# Drop merge indicator column\n",
    "integrated_data = integrated_data.drop('_merge', axis=1)\n",
    "\n",
    "print(integrated_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Validation Checks\n",
    "\n",
    "Run validation checks for duplicates, missing values, and data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-20T15:00:44.940320] Validation Start: Running final validation checks on integrated data\n"
     ]
    }
   ],
   "source": [
    "# log\n",
    "log_step('Validation Start', 'Running final validation checks on integrated data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 16\n",
      "         city country  population   aqi\n",
      "116    Suzhou   China   5324476.0  21.0\n",
      "117    Suzhou   China   5324476.0  36.0\n",
      "150   Taizhou   China   4512762.0  35.0\n",
      "151   Taizhou   China   4512762.0  38.0\n",
      "163    Suzhou   China   4330000.0  21.0\n",
      "164    Suzhou   China   4330000.0  36.0\n",
      "208    Fuzhou   China   3671192.0  68.0\n",
      "209    Fuzhou   China   3671192.0  50.0\n",
      "213    Fuzhou   China   3614866.0  68.0\n",
      "214    Fuzhou   China   3614866.0  50.0\n",
      "219  Baicheng   China   3571505.0  80.0\n",
      "220  Baicheng   China   3571505.0  77.0\n",
      "391   Taizhou   China   2162461.0  35.0\n",
      "392   Taizhou   China   2162461.0  38.0\n",
      "505  Baicheng   China   1551378.0  80.0\n",
      "506  Baicheng   China   1551378.0  77.0\n",
      "Removed 16 duplicates\n",
      "[2025-11-20T15:00:59.139596] Validation - Duplicates: Removed 16 duplicate records\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = integrated_data.duplicated(subset=['city', 'country'], keep=False)\n",
    "num_dups = duplicates.sum()\n",
    "\n",
    "print(f\"Duplicates: {num_dups}\")\n",
    "\n",
    "if num_dups > 0:\n",
    "    print(integrated_data[duplicates][['city', 'country', 'population', 'aqi']])\n",
    "    # Remove duplicates, keeping first occurrence\n",
    "    integrated_data = integrated_data.drop_duplicates(subset=['city', 'country'], keep='first')\n",
    "    print(f\"Removed {num_dups} duplicates\")\n",
    "    # log\n",
    "    log_step('Validation - Duplicates', f'Removed {num_dups} duplicate records')\n",
    "else:\n",
    "    # log\n",
    "    log_step('Validation - Duplicates', 'No duplicates found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aqi                   54\n",
      "aqi_category          54\n",
      "dominant_pollutant    54\n",
      "dtype: int64\n",
      "[2025-11-20T15:01:13.310244] Validation - Missing Values: Missing value counts: {'aqi': 54, 'aqi_category': 54, 'dominant_pollutant': 54}\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing_summary = integrated_data.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "missing_dict = missing_summary[missing_summary > 0].to_dict()\n",
    "# log\n",
    "log_step('Validation - Missing Values', f'Missing value counts: {missing_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population: 1,543,000 to 37,785,000\n",
      "AQI: 0.0 to 98.0\n",
      "Lat: -37.81 to 59.94\n",
      "Lon: -123.10 to 153.03\n",
      "Invalid coords: 0\n",
      "[2025-11-20T15:01:28.806860] Validation - Data Ranges: All data ranges validated\n"
     ]
    }
   ],
   "source": [
    "# Check data ranges\n",
    "print(f\"Population: {integrated_data['population'].min():,.0f} to {integrated_data['population'].max():,.0f}\")\n",
    "print(f\"AQI: {integrated_data['aqi'].min():.1f} to {integrated_data['aqi'].max():.1f}\")\n",
    "print(f\"Lat: {integrated_data['latitude'].min():.2f} to {integrated_data['latitude'].max():.2f}\")\n",
    "print(f\"Lon: {integrated_data['longitude'].min():.2f} to {integrated_data['longitude'].max():.2f}\")\n",
    "\n",
    "# Check for invalid coordinates\n",
    "invalid_coords = integrated_data[\n",
    "    (integrated_data['latitude'] < -90) | \n",
    "    (integrated_data['latitude'] > 90) |\n",
    "    (integrated_data['longitude'] < -180) | \n",
    "    (integrated_data['longitude'] > 180)\n",
    "]\n",
    "print(f\"Invalid coords: {len(invalid_coords)}\")\n",
    "\n",
    "# log\n",
    "log_step('Validation - Data Ranges', 'All data ranges validated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness:\n",
      "  city: 100.0%\n",
      "  country: 100.0%\n",
      "  latitude: 100.0%\n",
      "  longitude: 100.0%\n",
      "  population: 100.0%\n",
      "  iso2: 100.0%\n",
      "  iso3: 100.0%\n",
      "  aqi: 89.1%\n",
      "  aqi_category: 89.1%\n",
      "  dominant_pollutant: 89.1%\n",
      "  collection_timestamp: 100.0%\n",
      "  data_quality_flag: 100.0%\n",
      "[2025-11-20T15:01:44.359612] Validation - Completeness: Overall data completeness calculated\n"
     ]
    }
   ],
   "source": [
    "# completeness\n",
    "completeness = (1 - integrated_data.isnull().sum() / len(integrated_data)) * 100\n",
    "print(\"Completeness:\")\n",
    "for col in integrated_data.columns:\n",
    "    print(f\"  {col}: {completeness[col]:.1f}%\")\n",
    "\n",
    "# log\n",
    "log_step('Validation - Completeness', f'Overall data completeness calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Final Dataset Creation\n",
    "\n",
    "Create final clean, integrated CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (496, 12)\n",
      "        city    country iso2 iso3  latitude  longitude  population   aqi  \\\n",
      "0      Tokyo      Japan   JP  JPN   35.6870   139.7495  37785000.0  64.0   \n",
      "1    Jakarta  Indonesia   ID  IDN   -6.1750   106.8275  33756000.0  63.0   \n",
      "2      Delhi      India   IN  IND   28.6100    77.2300  32226000.0  14.0   \n",
      "3  Guangzhou      China   CN  CHN   23.1300   113.2600  26940000.0  60.0   \n",
      "4     Mumbai      India   IN  IND   19.0761    72.8775  24973000.0  19.0   \n",
      "\n",
      "       aqi_category dominant_pollutant data_quality_flag  \\\n",
      "0  Good air quality               pm25          complete   \n",
      "1  Good air quality               pm10          complete   \n",
      "2  Poor air quality               pm10          complete   \n",
      "3  Good air quality               pm10          complete   \n",
      "4  Poor air quality               pm10          complete   \n",
      "\n",
      "         collection_timestamp  \n",
      "0  2025-11-20T14:38:31.279676  \n",
      "1  2025-11-20T14:38:32.165895  \n",
      "2  2025-11-20T14:38:32.973334  \n",
      "3  2025-11-20T14:38:33.801853  \n",
      "4  2025-11-20T14:38:34.638730  \n"
     ]
    }
   ],
   "source": [
    "# reorder\n",
    "column_order = [\n",
    "    'city', 'country', 'iso2', 'iso3',\n",
    "    'latitude', 'longitude',\n",
    "    'population',\n",
    "    'aqi', 'aqi_category', 'dominant_pollutant',\n",
    "    'data_quality_flag', 'collection_timestamp'\n",
    "]\n",
    "\n",
    "final_data = integrated_data[column_order]\n",
    "\n",
    "print(f\"Final shape: {final_data.shape}\")\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/integrated_cities_air_quality_final.csv\n",
      "[2025-11-20T15:03:47.068251] Final Dataset: Created final integrated dataset: data/integrated_cities_air_quality_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Save final integrated dataset\n",
    "final_filename = 'data/integrated_cities_air_quality_final.csv'\n",
    "final_data.to_csv(final_filename, index=False)\n",
    "\n",
    "print(f\"Saved: {final_filename}\")\n",
    "# log\n",
    "log_step('Final Dataset', f'Created final integrated dataset: {final_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Curation Log Export\n",
    "\n",
    "Export all curation steps for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log saved: data/curation_log_20251120_150359.csv (25 steps)\n",
      "                    timestamp                          step  \\\n",
      "0  2025-11-20T14:26:27.387966                     Data Load   \n",
      "1  2025-11-20T14:26:46.632955   Validation - Missing Values   \n",
      "2  2025-11-20T14:28:21.702501       Validation - Duplicates   \n",
      "3  2025-11-20T14:28:32.216926  Validation - Population Data   \n",
      "4  2025-11-20T14:28:54.540540      Validation - Coordinates   \n",
      "5  2025-11-20T14:29:13.695526                Data Selection   \n",
      "6  2025-11-20T14:33:56.297092          API Collection Start   \n",
      "7  2025-11-20T14:38:30.595584          API Collection Start   \n",
      "8  2025-11-20T14:45:29.336727       API Collection Complete   \n",
      "9  2025-11-20T14:45:43.119919                  Data Storage   \n",
      "\n",
      "                                             details  \n",
      "0        Loaded 48059 cities from SimpleMaps dataset  \n",
      "1                      5 columns have missing values  \n",
      "2     Found 3694 duplicate city-country combinations  \n",
      "3               99.5% of cities have population data  \n",
      "4                  0 cities with invalid coordinates  \n",
      "5    Selected and saved top 500 cities by population  \n",
      "6                Beginning collection for 500 cities  \n",
      "7                Beginning collection for 500 cities  \n",
      "8         Collected 500 records in 418.7s, 54 errors  \n",
      "9  Saved raw API data: data/raw_air_quality_20251...  \n"
     ]
    }
   ],
   "source": [
    "# Save curation log\n",
    "log_df = pd.DataFrame(curation_log)\n",
    "log_filename = f\"data/curation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "log_df.to_csv(log_filename, index=False)\n",
    "\n",
    "print(f\"Log saved: {log_filename} ({len(curation_log)} steps)\")\n",
    "print(log_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3/4 Complete\n",
    "\n",
    "Successfully completed:\n",
    "- ✓ SimpleMaps dataset validation\n",
    "- ✓ Large-scale API data collection (500 cities)\n",
    "- ✓ Error handling and rate limiting\n",
    "- ✓ Raw data storage with timestamps\n",
    "- ✓ Data cleaning and standardization\n",
    "- ✓ Dataset integration and merging\n",
    "- ✓ Validation checks (duplicates, missing values, ranges)\n",
    "- ✓ Final CSV creation\n",
    "- ✓ Curation log documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
